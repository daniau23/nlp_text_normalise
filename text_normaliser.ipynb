{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import nltk\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import gutenberg as cg\n",
    "# sentence tokeniser\n",
    "from nltk.tokenize import sent_tokenize as st \n",
    "# word tokeniser\n",
    "from nltk.tokenize import word_tokenize as wt \n",
    "from nltk.book import FreqDist\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is aimed at developing a preprocessing process for **TEXT NORMALISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps of text normalisation**\n",
    "\n",
    "- lowercase\n",
    "- removal of stopwords\n",
    "- remove punctuations\n",
    "- remove numbers\n",
    "- tokenisation\n",
    "- lemmatisation/stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take in raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the conference, within the machine learning session, Xiu Tang, Sai Wu, Gang Chen, Ke Chen, and Lidan Shou of College of Computer Science and Technology, Zhejiang University, Zhejiang, China, gave a presentation on the “Learning to Label with Active Learning and Reinforcement Learning”.  They pointed out that it is financially training data labelling in domain-specific learning applications, which relies on the intelligence from domain experts. They proposed a learning-to-label (L2L) framework leveraging active learning and reinforcement learning to iteratively select data to label for Name Entity Recognition (NER) task. They pointed out that, neural models are built on top of few open datasets with well-defined labels, such as; ImageNet, Coco, and Wikipedia dataset, which cannot be directly applied to new domain. This means that neural model using domain-specific labels must be trained, but this is quite expensive. To address the problem of lacking labeled training data, they proposed the use of transfer learning technique which establishes a model on the source domain and transfers the knowledge to a target domain, but with an extension by exploiting active learning and reinforcement learning techniques. By use of a label approach, L2L, which consists of two models, a transfer learning model and an active learning model designed by a reinforcement learning process named T-model and A-model, respectively, to rank the data for labelling for the Name Entity Recognition (NER) application. The L2L architecture consisted of three main components which are: NER model, multi-granularity attention, and learning to rank. The proposed architecture reduced the required number of labels for training a domain-specific neural model. The idea of this model is to first transfer a learning model from a source domain to a target domain, and then apply the active learning to gradually improve the performance of the model using as few labeled data in the target domain as possible. Their experimental results showed that their approach is more effective than strong previous methods using heuristics and reinforcement learning. With the same number of labeled data, their approach improved the accuracy of NER by 11.91%. Moreover, this approach is superior to state-of-the-art learning- to-label method, with an improvement of accuracy by 6.49%.\n"
     ]
    }
   ],
   "source": [
    "raw_text=\"\"\"During the conference, within the machine learning session, Xiu Tang, Sai Wu, Gang Chen, Ke Chen, and Lidan Shou of College of Computer Science and Technology, Zhejiang University, Zhejiang, China, gave a presentation on the “Learning to Label with Active Learning and Reinforcement Learning”.  They pointed out that it is financially training data labelling in domain-specific learning applications, which relies on the intelligence from domain experts. They proposed a learning-to-label (L2L) framework leveraging active learning and reinforcement learning to iteratively select data to label for Name Entity Recognition (NER) task. They pointed out that, neural models are built on top of few open datasets with well-defined labels, such as; ImageNet, Coco, and Wikipedia dataset, which cannot be directly applied to new domain. This means that neural model using domain-specific labels must be trained, but this is quite expensive. To address the problem of lacking labeled training data, they proposed the use of transfer learning technique which establishes a model on the source domain and transfers the knowledge to a target domain, but with an extension by exploiting active learning and reinforcement learning techniques. By use of a label approach, L2L, which consists of two models, a transfer learning model and an active learning model designed by a reinforcement learning process named T-model and A-model, respectively, to rank the data for labelling for the Name Entity Recognition (NER) application. The L2L architecture consisted of three main components which are: NER model, multi-granularity attention, and learning to rank. The proposed architecture reduced the required number of labels for training a domain-specific neural model. The idea of this model is to first transfer a learning model from a source domain to a target domain, and then apply the active learning to gradually improve the performance of the model using as few labeled data in the target domain as possible. Their experimental results showed that their approach is more effective than strong previous methods using heuristics and reinforcement learning. With the same number of labeled data, their approach improved the accuracy of NER by 11.91%. Moreover, this approach is superior to state-of-the-art learning- to-label method, with an improvement of accuracy by 6.49%.\"\"\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('During', 'IN'), ('the', 'DT'), ('conference', 'NN'), (',', ','), ('within', 'IN'), ('the', 'DT'), ('machine', 'NN'), ('learning', 'NN'), ('session', 'NN'), (',', ','), ('Xiu', 'NNP'), ('Tang', 'NNP'), (',', ','), ('Sai', 'NNP'), ('Wu', 'NNP'), (',', ','), ('Gang', 'NNP'), ('Chen', 'NNP'), (',', ','), ('Ke', 'NNP'), ('Chen', 'NNP'), (',', ','), ('and', 'CC'), ('Lidan', 'NNP'), ('Shou', 'NNP'), ('of', 'IN'), ('College', 'NNP'), ('of', 'IN'), ('Computer', 'NNP'), ('Science', 'NNP'), ('and', 'CC'), ('Technology', 'NNP'), (',', ','), ('Zhejiang', 'NNP'), ('University', 'NNP'), (',', ','), ('Zhejiang', 'NNP'), (',', ','), ('China', 'NNP'), (',', ','), ('gave', 'VBD'), ('a', 'DT'), ('presentation', 'NN'), ('on', 'IN'), ('the', 'DT'), ('“', 'NN'), ('Learning', 'NNP'), ('to', 'TO'), ('Label', 'NNP'), ('with', 'IN'), ('Active', 'NNP'), ('Learning', 'NNP'), ('and', 'CC'), ('Reinforcement', 'NNP'), ('Learning', 'NNP'), ('”', 'NNP'), ('.', '.'), ('They', 'PRP'), ('pointed', 'VBD'), ('out', 'RP'), ('that', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('financially', 'RB'), ('training', 'VBG'), ('data', 'NNS'), ('labelling', 'VBG'), ('in', 'IN'), ('domain-specific', 'JJ'), ('learning', 'NN'), ('applications', 'NNS'), (',', ','), ('which', 'WDT'), ('relies', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('intelligence', 'NN'), ('from', 'IN'), ('domain', 'NN'), ('experts', 'NNS'), ('.', '.'), ('They', 'PRP'), ('proposed', 'VBD'), ('a', 'DT'), ('learning-to-label', 'JJ'), ('(', '('), ('L2L', 'NNP'), (')', ')'), ('framework', 'NN'), ('leveraging', 'VBG'), ('active', 'JJ'), ('learning', 'NN'), ('and', 'CC'), ('reinforcement', 'NN'), ('learning', 'NN'), ('to', 'TO'), ('iteratively', 'RB'), ('select', 'VB'), ('data', 'NNS'), ('to', 'TO'), ('label', 'VB'), ('for', 'IN'), ('Name', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('task', 'NN'), ('.', '.'), ('They', 'PRP'), ('pointed', 'VBD'), ('out', 'RP'), ('that', 'IN'), (',', ','), ('neural', 'JJ'), ('models', 'NNS'), ('are', 'VBP'), ('built', 'VBN'), ('on', 'IN'), ('top', 'NN'), ('of', 'IN'), ('few', 'JJ'), ('open', 'JJ'), ('datasets', 'NNS'), ('with', 'IN'), ('well-defined', 'JJ'), ('labels', 'NNS'), (',', ','), ('such', 'JJ'), ('as', 'IN'), (';', ':'), ('ImageNet', 'NNP'), (',', ','), ('Coco', 'NNP'), (',', ','), ('and', 'CC'), ('Wikipedia', 'NNP'), ('dataset', 'NN'), (',', ','), ('which', 'WDT'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('directly', 'RB'), ('applied', 'VBN'), ('to', 'TO'), ('new', 'JJ'), ('domain', 'NN'), ('.', '.'), ('This', 'DT'), ('means', 'VBZ'), ('that', 'IN'), ('neural', 'JJ'), ('model', 'NN'), ('using', 'VBG'), ('domain-specific', 'JJ'), ('labels', 'NNS'), ('must', 'MD'), ('be', 'VB'), ('trained', 'VBN'), (',', ','), ('but', 'CC'), ('this', 'DT'), ('is', 'VBZ'), ('quite', 'RB'), ('expensive', 'JJ'), ('.', '.'), ('To', 'TO'), ('address', 'VB'), ('the', 'DT'), ('problem', 'NN'), ('of', 'IN'), ('lacking', 'VBG'), ('labeled', 'JJ'), ('training', 'NN'), ('data', 'NNS'), (',', ','), ('they', 'PRP'), ('proposed', 'VBD'), ('the', 'DT'), ('use', 'NN'), ('of', 'IN'), ('transfer', 'NN'), ('learning', 'NN'), ('technique', 'NN'), ('which', 'WDT'), ('establishes', 'VBZ'), ('a', 'DT'), ('model', 'NN'), ('on', 'IN'), ('the', 'DT'), ('source', 'NN'), ('domain', 'NN'), ('and', 'CC'), ('transfers', 'VBZ'), ('the', 'DT'), ('knowledge', 'NN'), ('to', 'TO'), ('a', 'DT'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('but', 'CC'), ('with', 'IN'), ('an', 'DT'), ('extension', 'NN'), ('by', 'IN'), ('exploiting', 'VBG'), ('active', 'JJ'), ('learning', 'NN'), ('and', 'CC'), ('reinforcement', 'NN'), ('learning', 'NN'), ('techniques', 'NNS'), ('.', '.'), ('By', 'IN'), ('use', 'NN'), ('of', 'IN'), ('a', 'DT'), ('label', 'NN'), ('approach', 'NN'), (',', ','), ('L2L', 'NNP'), (',', ','), ('which', 'WDT'), ('consists', 'VBZ'), ('of', 'IN'), ('two', 'CD'), ('models', 'NNS'), (',', ','), ('a', 'DT'), ('transfer', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('and', 'CC'), ('an', 'DT'), ('active', 'JJ'), ('learning', 'NN'), ('model', 'NN'), ('designed', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('process', 'NN'), ('named', 'VBN'), ('T-model', 'NNP'), ('and', 'CC'), ('A-model', 'NNP'), (',', ','), ('respectively', 'RB'), (',', ','), ('to', 'TO'), ('rank', 'VB'), ('the', 'DT'), ('data', 'NN'), ('for', 'IN'), ('labelling', 'VBG'), ('for', 'IN'), ('the', 'DT'), ('Name', 'NNP'), ('Entity', 'NNP'), ('Recognition', 'NNP'), ('(', '('), ('NER', 'NNP'), (')', ')'), ('application', 'NN'), ('.', '.'), ('The', 'DT'), ('L2L', 'NNP'), ('architecture', 'NN'), ('consisted', 'VBD'), ('of', 'IN'), ('three', 'CD'), ('main', 'JJ'), ('components', 'NNS'), ('which', 'WDT'), ('are', 'VBP'), (':', ':'), ('NER', 'NNP'), ('model', 'NN'), (',', ','), ('multi-granularity', 'JJ'), ('attention', 'NN'), (',', ','), ('and', 'CC'), ('learning', 'VBG'), ('to', 'TO'), ('rank', 'VB'), ('.', '.'), ('The', 'DT'), ('proposed', 'JJ'), ('architecture', 'NN'), ('reduced', 'VBD'), ('the', 'DT'), ('required', 'JJ'), ('number', 'NN'), ('of', 'IN'), ('labels', 'NNS'), ('for', 'IN'), ('training', 'VBG'), ('a', 'DT'), ('domain-specific', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('.', '.'), ('The', 'DT'), ('idea', 'NN'), ('of', 'IN'), ('this', 'DT'), ('model', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('first', 'JJ'), ('transfer', 'VB'), ('a', 'DT'), ('learning', 'JJ'), ('model', 'NN'), ('from', 'IN'), ('a', 'DT'), ('source', 'NN'), ('domain', 'NN'), ('to', 'TO'), ('a', 'DT'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('and', 'CC'), ('then', 'RB'), ('apply', 'VB'), ('the', 'DT'), ('active', 'JJ'), ('learning', 'NN'), ('to', 'TO'), ('gradually', 'RB'), ('improve', 'VB'), ('the', 'DT'), ('performance', 'NN'), ('of', 'IN'), ('the', 'DT'), ('model', 'NN'), ('using', 'VBG'), ('as', 'IN'), ('few', 'JJ'), ('labeled', 'VBN'), ('data', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('target', 'NN'), ('domain', 'NN'), ('as', 'IN'), ('possible', 'JJ'), ('.', '.'), ('Their', 'PRP$'), ('experimental', 'JJ'), ('results', 'NNS'), ('showed', 'VBD'), ('that', 'IN'), ('their', 'PRP$'), ('approach', 'NN'), ('is', 'VBZ'), ('more', 'RBR'), ('effective', 'JJ'), ('than', 'IN'), ('strong', 'JJ'), ('previous', 'JJ'), ('methods', 'NNS'), ('using', 'VBG'), ('heuristics', 'NNS'), ('and', 'CC'), ('reinforcement', 'NN'), ('learning', 'NN'), ('.', '.'), ('With', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('number', 'NN'), ('of', 'IN'), ('labeled', 'JJ'), ('data', 'NNS'), (',', ','), ('their', 'PRP$'), ('approach', 'NN'), ('improved', 'VBD'), ('the', 'DT'), ('accuracy', 'NN'), ('of', 'IN'), ('NER', 'NNP'), ('by', 'IN'), ('11.91', 'CD'), ('%', 'NN'), ('.', '.'), ('Moreover', 'RB'), (',', ','), ('this', 'DT'), ('approach', 'NN'), ('is', 'VBZ'), ('superior', 'JJ'), ('to', 'TO'), ('state-of-the-art', 'JJ'), ('learning-', 'JJ'), ('to-label', 'NN'), ('method', 'NN'), (',', ','), ('with', 'IN'), ('an', 'DT'), ('improvement', 'NN'), ('of', 'IN'), ('accuracy', 'NN'), ('by', 'IN'), ('6.49', 'CD'), ('%', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Eexplore the tags of the text\n",
    "print(pos_tag(wt(raw_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function definitions for text normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordlowercase(raw_text):\n",
    "    return raw_text.lower()\n",
    "\n",
    "def stopwordremove2(sentence):\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    removed_stopwords_text = \" \".join(i for i in sentence.split() if i not in stop)\n",
    "    return removed_stopwords_text\n",
    "\n",
    "\n",
    "def word_tokenise2(removed_stopwords_text):\n",
    "    return wt(removed_stopwords_text)\n",
    "\n",
    "\n",
    "def stemmer_porter(word_tokens):\n",
    "    port = PorterStemmer()\n",
    "    # stemmed = \" \".join([port.stem(i) for i in word_tokens])\n",
    "    stemmed = [port.stem(i) for i in word_tokens]\n",
    "    return stemmed\n",
    "\n",
    "'''Individual Lemmatisation'''\n",
    "def lemmtiser_verb(sentence):\n",
    "    lemmaword = WordNetLemmatizer()\n",
    "    # lemma_text = \" \".join(lemmaword.lemmatize(i, \"v\") for i in sentence.split()) # only verbs\n",
    "    lemma_text = [lemmaword.lemmatize(i, \"v\") for i in sentence] # only verbs\n",
    "    return lemma_text\n",
    "\n",
    "def lemmtiser_adjective(sentence):\n",
    "    lemmaword = WordNetLemmatizer()\n",
    "    # lemma_text = \" \".join(lemmaword.lemmatize(i, \"a\") for i in sentence.split()) # only adjective\n",
    "    lemma_text = [lemmaword.lemmatize(i, \"a\") for i in sentence] # only adjective\n",
    "    return lemma_text\n",
    "def lemmtiser_noun(sentence):\n",
    "    lemmaword = WordNetLemmatizer()\n",
    "    # lemma_text = \" \".join(lemmaword.lemmatize(i, \"n\") for i in sentence.split()) # only noun\n",
    "    lemma_text = [lemmaword.lemmatize(i, \"n\") for i in sentence]# only noun\n",
    "    return lemma_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercase text\n",
      "during the conference, within the machine learning session, xiu tang, sai wu, gang chen, ke chen, and lidan shou of college of computer science and technology, zhejiang university, zhejiang, china, gave a presentation on the “learning to label with active learning and reinforcement learning”.  they pointed out that it is financially training data labelling in domain-specific learning applications, which relies on the intelligence from domain experts. they proposed a learning-to-label (l2l) framework leveraging active learning and reinforcement learning to iteratively select data to label for name entity recognition (ner) task. they pointed out that, neural models are built on top of few open datasets with well-defined labels, such as; imagenet, coco, and wikipedia dataset, which cannot be directly applied to new domain. this means that neural model using domain-specific labels must be trained, but this is quite expensive. to address the problem of lacking labeled training data, they proposed the use of transfer learning technique which establishes a model on the source domain and transfers the knowledge to a target domain, but with an extension by exploiting active learning and reinforcement learning techniques. by use of a label approach, l2l, which consists of two models, a transfer learning model and an active learning model designed by a reinforcement learning process named t-model and a-model, respectively, to rank the data for labelling for the name entity recognition (ner) application. the l2l architecture consisted of three main components which are: ner model, multi-granularity attention, and learning to rank. the proposed architecture reduced the required number of labels for training a domain-specific neural model. the idea of this model is to first transfer a learning model from a source domain to a target domain, and then apply the active learning to gradually improve the performance of the model using as few labeled data in the target domain as possible. their experimental results showed that their approach is more effective than strong previous methods using heuristics and reinforcement learning. with the same number of labeled data, their approach improved the accuracy of ner by 11.91%. moreover, this approach is superior to state-of-the-art learning- to-label method, with an improvement of accuracy by 6.49%.\n",
      "\n",
      "stop word removal:\n",
      "conference, within machine learning session, xiu tang, sai wu, gang chen, ke chen, lidan shou college computer science technology, zhejiang university, zhejiang, china, gave presentation “learning label active learning reinforcement learning”. pointed financially training data labelling domain-specific learning applications, relies intelligence domain experts. proposed learning-to-label (l2l) framework leveraging active learning reinforcement learning iteratively select data label name entity recognition (ner) task. pointed that, neural models built top open datasets well-defined labels, as; imagenet, coco, wikipedia dataset, cannot directly applied new domain. means neural model using domain-specific labels must trained, quite expensive. address problem lacking labeled training data, proposed use transfer learning technique establishes model source domain transfers knowledge target domain, extension exploiting active learning reinforcement learning techniques. use label approach, l2l, consists two models, transfer learning model active learning model designed reinforcement learning process named t-model a-model, respectively, rank data labelling name entity recognition (ner) application. l2l architecture consisted three main components are: ner model, multi-granularity attention, learning rank. proposed architecture reduced required number labels training domain-specific neural model. idea model first transfer learning model source domain target domain, apply active learning gradually improve performance model using labeled data target domain possible. experimental results showed approach effective strong previous methods using heuristics reinforcement learning. number labeled data, approach improved accuracy ner 11.91%. moreover, approach superior state-of-the-art learning- to-label method, improvement accuracy 6.49%.\n",
      "\n",
      "Tokenised lowercase text:\n",
      "['conference', ',', 'within', 'machine', 'learning', 'session', ',', 'xiu', 'tang', ',', 'sai', 'wu', ',', 'gang', 'chen', ',', 'ke', 'chen', ',', 'lidan', 'shou', 'college', 'computer', 'science', 'technology', ',', 'zhejiang', 'university', ',', 'zhejiang', ',', 'china', ',', 'gave', 'presentation', '“', 'learning', 'label', 'active', 'learning', 'reinforcement', 'learning', '”', '.', 'pointed', 'financially', 'training', 'data', 'labelling', 'domain-specific', 'learning', 'applications', ',', 'relies', 'intelligence', 'domain', 'experts', '.', 'proposed', 'learning-to-label', '(', 'l2l', ')', 'framework', 'leveraging', 'active', 'learning', 'reinforcement', 'learning', 'iteratively', 'select', 'data', 'label', 'name', 'entity', 'recognition', '(', 'ner', ')', 'task', '.', 'pointed', 'that', ',', 'neural', 'models', 'built', 'top', 'open', 'datasets', 'well-defined', 'labels', ',', 'as', ';', 'imagenet', ',', 'coco', ',', 'wikipedia', 'dataset', ',', 'can', 'not', 'directly', 'applied', 'new', 'domain', '.', 'means', 'neural', 'model', 'using', 'domain-specific', 'labels', 'must', 'trained', ',', 'quite', 'expensive', '.', 'address', 'problem', 'lacking', 'labeled', 'training', 'data', ',', 'proposed', 'use', 'transfer', 'learning', 'technique', 'establishes', 'model', 'source', 'domain', 'transfers', 'knowledge', 'target', 'domain', ',', 'extension', 'exploiting', 'active', 'learning', 'reinforcement', 'learning', 'techniques', '.', 'use', 'label', 'approach', ',', 'l2l', ',', 'consists', 'two', 'models', ',', 'transfer', 'learning', 'model', 'active', 'learning', 'model', 'designed', 'reinforcement', 'learning', 'process', 'named', 't-model', 'a-model', ',', 'respectively', ',', 'rank', 'data', 'labelling', 'name', 'entity', 'recognition', '(', 'ner', ')', 'application', '.', 'l2l', 'architecture', 'consisted', 'three', 'main', 'components', 'are', ':', 'ner', 'model', ',', 'multi-granularity', 'attention', ',', 'learning', 'rank', '.', 'proposed', 'architecture', 'reduced', 'required', 'number', 'labels', 'training', 'domain-specific', 'neural', 'model', '.', 'idea', 'model', 'first', 'transfer', 'learning', 'model', 'source', 'domain', 'target', 'domain', ',', 'apply', 'active', 'learning', 'gradually', 'improve', 'performance', 'model', 'using', 'labeled', 'data', 'target', 'domain', 'possible', '.', 'experimental', 'results', 'showed', 'approach', 'effective', 'strong', 'previous', 'methods', 'using', 'heuristics', 'reinforcement', 'learning', '.', 'number', 'labeled', 'data', ',', 'approach', 'improved', 'accuracy', 'ner', '11.91', '%', '.', 'moreover', ',', 'approach', 'superior', 'state-of-the-art', 'learning-', 'to-label', 'method', ',', 'improvement', 'accuracy', '6.49', '%', '.']\n",
      "\n",
      "Stemmed lowercase text:\n",
      "['confer', ',', 'within', 'machin', 'learn', 'session', ',', 'xiu', 'tang', ',', 'sai', 'wu', ',', 'gang', 'chen', ',', 'ke', 'chen', ',', 'lidan', 'shou', 'colleg', 'comput', 'scienc', 'technolog', ',', 'zhejiang', 'univers', ',', 'zhejiang', ',', 'china', ',', 'gave', 'present', '“', 'learn', 'label', 'activ', 'learn', 'reinforc', 'learn', '”', '.', 'point', 'financi', 'train', 'data', 'label', 'domain-specif', 'learn', 'applic', ',', 'reli', 'intellig', 'domain', 'expert', '.', 'propos', 'learning-to-label', '(', 'l2l', ')', 'framework', 'leverag', 'activ', 'learn', 'reinforc', 'learn', 'iter', 'select', 'data', 'label', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'task', '.', 'point', 'that', ',', 'neural', 'model', 'built', 'top', 'open', 'dataset', 'well-defin', 'label', ',', 'as', ';', 'imagenet', ',', 'coco', ',', 'wikipedia', 'dataset', ',', 'can', 'not', 'directli', 'appli', 'new', 'domain', '.', 'mean', 'neural', 'model', 'use', 'domain-specif', 'label', 'must', 'train', ',', 'quit', 'expens', '.', 'address', 'problem', 'lack', 'label', 'train', 'data', ',', 'propos', 'use', 'transfer', 'learn', 'techniqu', 'establish', 'model', 'sourc', 'domain', 'transfer', 'knowledg', 'target', 'domain', ',', 'extens', 'exploit', 'activ', 'learn', 'reinforc', 'learn', 'techniqu', '.', 'use', 'label', 'approach', ',', 'l2l', ',', 'consist', 'two', 'model', ',', 'transfer', 'learn', 'model', 'activ', 'learn', 'model', 'design', 'reinforc', 'learn', 'process', 'name', 't-model', 'a-model', ',', 'respect', ',', 'rank', 'data', 'label', 'name', 'entiti', 'recognit', '(', 'ner', ')', 'applic', '.', 'l2l', 'architectur', 'consist', 'three', 'main', 'compon', 'are', ':', 'ner', 'model', ',', 'multi-granular', 'attent', ',', 'learn', 'rank', '.', 'propos', 'architectur', 'reduc', 'requir', 'number', 'label', 'train', 'domain-specif', 'neural', 'model', '.', 'idea', 'model', 'first', 'transfer', 'learn', 'model', 'sourc', 'domain', 'target', 'domain', ',', 'appli', 'activ', 'learn', 'gradual', 'improv', 'perform', 'model', 'use', 'label', 'data', 'target', 'domain', 'possibl', '.', 'experiment', 'result', 'show', 'approach', 'effect', 'strong', 'previou', 'method', 'use', 'heurist', 'reinforc', 'learn', '.', 'number', 'label', 'data', ',', 'approach', 'improv', 'accuraci', 'ner', '11.91', '%', '.', 'moreov', ',', 'approach', 'superior', 'state-of-the-art', 'learning-', 'to-label', 'method', ',', 'improv', 'accuraci', '6.49', '%', '.']\n",
      "\n",
      "Stemmed lowercase sentence:\n",
      "confer , within machin learn session , xiu tang , sai wu , gang chen , ke chen , lidan shou colleg comput scienc technolog , zhejiang univers , zhejiang , china , gave present “ learn label activ learn reinforc learn ” . point financi train data label domain-specif learn applic , reli intellig domain expert . propos learning-to-label ( l2l ) framework leverag activ learn reinforc learn iter select data label name entiti recognit ( ner ) task . point that , neural model built top open dataset well-defin label , as ; imagenet , coco , wikipedia dataset , can not directli appli new domain . mean neural model use domain-specif label must train , quit expens . address problem lack label train data , propos use transfer learn techniqu establish model sourc domain transfer knowledg target domain , extens exploit activ learn reinforc learn techniqu . use label approach , l2l , consist two model , transfer learn model activ learn model design reinforc learn process name t-model a-model , respect , rank data label name entiti recognit ( ner ) applic . l2l architectur consist three main compon are : ner model , multi-granular attent , learn rank . propos architectur reduc requir number label train domain-specif neural model . idea model first transfer learn model sourc domain target domain , appli activ learn gradual improv perform model use label data target domain possibl . experiment result show approach effect strong previou method use heurist reinforc learn . number label data , approach improv accuraci ner 11.91 % . moreov , approach superior state-of-the-art learning- to-label method , improv accuraci 6.49 % .\n",
      "\n",
      "POS tagging lowercase text:\n",
      "[('confer', 'NN'), (',', ','), ('within', 'IN'), ('machin', 'JJ'), ('learn', 'NN'), ('session', 'NN'), (',', ','), ('xiu', 'NNP'), ('tang', 'NN'), (',', ','), ('sai', 'NN'), ('wu', 'NN'), (',', ','), ('gang', 'NN'), ('chen', 'NN'), (',', ','), ('ke', 'NN'), ('chen', 'NN'), (',', ','), ('lidan', 'JJ'), ('shou', 'NN'), ('colleg', 'NN'), ('comput', 'NN'), ('scienc', 'NN'), ('technolog', 'NN'), (',', ','), ('zhejiang', 'NN'), ('univers', 'NNS'), (',', ','), ('zhejiang', 'NN'), (',', ','), ('china', 'NN'), (',', ','), ('gave', 'VBD'), ('present', 'JJ'), ('“', 'NN'), ('learn', 'NN'), ('label', 'NN'), ('activ', 'NN'), ('learn', 'VBP'), ('reinforc', 'NN'), ('learn', 'NN'), ('”', 'NNP'), ('.', '.'), ('point', 'NN'), ('financi', 'NN'), ('train', 'NN'), ('data', 'NNS'), ('label', 'VBD'), ('domain-specif', 'NN'), ('learn', 'NN'), ('applic', 'NN'), (',', ','), ('reli', 'JJ'), ('intellig', 'NN'), ('domain', 'NN'), ('expert', 'NN'), ('.', '.'), ('propos', 'JJ'), ('learning-to-label', 'JJ'), ('(', '('), ('l2l', 'NN'), (')', ')'), ('framework', 'NN'), ('leverag', 'JJ'), ('activ', 'NN'), ('learn', 'NN'), ('reinforc', 'NN'), ('learn', 'VBP'), ('iter', 'NN'), ('select', 'NN'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entiti', 'JJ'), ('recognit', 'NN'), ('(', '('), ('ner', 'NN'), (')', ')'), ('task', 'NN'), ('.', '.'), ('point', 'NN'), ('that', 'IN'), (',', ','), ('neural', 'JJ'), ('model', 'NN'), ('built', 'VBN'), ('top', 'JJ'), ('open', 'JJ'), ('dataset', 'VBN'), ('well-defin', 'JJ'), ('label', 'NN'), (',', ','), ('as', 'IN'), (';', ':'), ('imagenet', 'NN'), (',', ','), ('coco', 'NN'), (',', ','), ('wikipedia', 'NN'), ('dataset', 'NN'), (',', ','), ('can', 'MD'), ('not', 'RB'), ('directli', 'VB'), ('appli', 'JJ'), ('new', 'JJ'), ('domain', 'NN'), ('.', '.'), ('mean', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('use', 'IN'), ('domain-specif', 'JJ'), ('label', 'NN'), ('must', 'MD'), ('train', 'VB'), (',', ','), ('quit', 'NN'), ('expens', 'NNS'), ('.', '.'), ('address', 'NN'), ('problem', 'NN'), ('lack', 'NN'), ('label', 'NN'), ('train', 'NN'), ('data', 'NNS'), (',', ','), ('propos', 'NN'), ('use', 'NN'), ('transfer', 'NN'), ('learn', 'NN'), ('techniqu', 'IN'), ('establish', 'VB'), ('model', 'NN'), ('sourc', 'NN'), ('domain', 'NN'), ('transfer', 'NN'), ('knowledg', 'NN'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('extens', 'VBZ'), ('exploit', 'JJ'), ('activ', 'NN'), ('learn', 'NN'), ('reinforc', 'NN'), ('learn', 'NN'), ('techniqu', 'NN'), ('.', '.'), ('use', 'NN'), ('label', 'JJ'), ('approach', 'NN'), (',', ','), ('l2l', 'NN'), (',', ','), ('consist', 'VBP'), ('two', 'CD'), ('model', 'NN'), (',', ','), ('transfer', 'NN'), ('learn', 'NN'), ('model', 'NN'), ('activ', 'NN'), ('learn', 'VBP'), ('model', 'NN'), ('design', 'NN'), ('reinforc', 'NN'), ('learn', 'NN'), ('process', 'NN'), ('name', 'IN'), ('t-model', 'JJ'), ('a-model', 'NN'), (',', ','), ('respect', 'NN'), (',', ','), ('rank', 'NN'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entiti', 'JJ'), ('recognit', 'NN'), ('(', '('), ('ner', 'NN'), (')', ')'), ('applic', 'NN'), ('.', '.'), ('l2l', 'JJ'), ('architectur', 'JJ'), ('consist', 'NN'), ('three', 'CD'), ('main', 'JJ'), ('compon', 'NN'), ('are', 'VBP'), (':', ':'), ('ner', 'JJ'), ('model', 'NN'), (',', ','), ('multi-granular', 'JJ'), ('attent', 'NN'), (',', ','), ('learn', 'JJ'), ('rank', 'NN'), ('.', '.'), ('propos', 'NN'), ('architectur', 'NN'), ('reduc', 'NN'), ('requir', 'NN'), ('number', 'NN'), ('label', 'NN'), ('train', 'VBP'), ('domain-specif', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('.', '.'), ('idea', 'NN'), ('model', 'NN'), ('first', 'JJ'), ('transfer', 'NN'), ('learn', 'NN'), ('model', 'NN'), ('sourc', 'NN'), ('domain', 'NN'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('appli', 'JJ'), ('activ', 'NN'), ('learn', 'VBP'), ('gradual', 'JJ'), ('improv', 'NN'), ('perform', 'NN'), ('model', 'NN'), ('use', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('target', 'NN'), ('domain', 'NN'), ('possibl', 'NN'), ('.', '.'), ('experiment', 'JJ'), ('result', 'NN'), ('show', 'NN'), ('approach', 'NN'), ('effect', 'NN'), ('strong', 'JJ'), ('previou', 'NN'), ('method', 'NN'), ('use', 'NN'), ('heurist', 'NN'), ('reinforc', 'NN'), ('learn', 'NN'), ('.', '.'), ('number', 'NN'), ('label', 'NN'), ('data', 'NNS'), (',', ','), ('approach', 'NN'), ('improv', 'NN'), ('accuraci', 'VBD'), ('ner', 'RB'), ('11.91', 'CD'), ('%', 'NN'), ('.', '.'), ('moreov', 'NN'), (',', ','), ('approach', 'NN'), ('superior', 'JJ'), ('state-of-the-art', 'JJ'), ('learning-', 'JJ'), ('to-label', 'NN'), ('method', 'NN'), (',', ','), ('improv', 'VB'), ('accuraci', 'RB'), ('6.49', 'CD'), ('%', 'NN'), ('.', '.')]\n",
      "\n",
      "Frequency of occurrence: \n",
      "[(',', 30), ('learn', 17), ('.', 13), ('label', 11), ('model', 11), ('domain', 7), ('data', 6), ('activ', 5), ('reinforc', 5), ('use', 5), ('train', 4), ('ner', 4), ('transfer', 4), ('approach', 4), ('domain-specif', 3), ('propos', 3), ('(', 3), ('l2l', 3), (')', 3), ('name', 3), ('neural', 3), ('target', 3), ('improv', 3), ('chen', 2), ('zhejiang', 2), ('point', 2), ('applic', 2), ('entiti', 2), ('recognit', 2), ('dataset', 2), ('appli', 2), ('techniqu', 2), ('sourc', 2), ('consist', 2), ('rank', 2), ('architectur', 2), ('number', 2), ('method', 2), ('accuraci', 2), ('%', 2), ('confer', 1), ('within', 1), ('machin', 1), ('session', 1), ('xiu', 1), ('tang', 1), ('sai', 1), ('wu', 1), ('gang', 1), ('ke', 1), ('lidan', 1), ('shou', 1), ('colleg', 1), ('comput', 1), ('scienc', 1), ('technolog', 1), ('univers', 1), ('china', 1), ('gave', 1), ('present', 1), ('“', 1), ('”', 1), ('financi', 1), ('reli', 1), ('intellig', 1), ('expert', 1), ('learning-to-label', 1), ('framework', 1), ('leverag', 1), ('iter', 1), ('select', 1), ('task', 1), ('that', 1), ('built', 1), ('top', 1), ('open', 1), ('well-defin', 1), ('as', 1), (';', 1), ('imagenet', 1), ('coco', 1), ('wikipedia', 1), ('can', 1), ('not', 1), ('directli', 1), ('new', 1), ('mean', 1), ('must', 1), ('quit', 1), ('expens', 1), ('address', 1), ('problem', 1), ('lack', 1), ('establish', 1), ('knowledg', 1), ('extens', 1), ('exploit', 1), ('two', 1), ('design', 1), ('process', 1), ('t-model', 1), ('a-model', 1), ('respect', 1), ('three', 1), ('main', 1), ('compon', 1), ('are', 1), (':', 1), ('multi-granular', 1), ('attent', 1), ('reduc', 1), ('requir', 1), ('idea', 1), ('first', 1), ('gradual', 1), ('perform', 1), ('possibl', 1), ('experiment', 1), ('result', 1), ('show', 1), ('effect', 1), ('strong', 1), ('previou', 1), ('heurist', 1), ('11.91', 1), ('moreov', 1), ('superior', 1), ('state-of-the-art', 1), ('learning-', 1), ('to-label', 1), ('6.49', 1)]\n"
     ]
    }
   ],
   "source": [
    "# lowercased text\n",
    "word_lowered = wordlowercase(raw_text)\n",
    "# removal of stopwords\n",
    "stop_wording = stopwordremove2(word_lowered)\n",
    "\n",
    "# tokenisation\n",
    "tokens = word_tokenise2(stop_wording)\n",
    "\n",
    "# Stemming\n",
    "stemmed = stemmer_porter(tokens)\n",
    "\n",
    "# Frequency of occurrence\n",
    "fdist3 = FreqDist(stemmed)\n",
    "\n",
    "print(f\"\\nLowercase text\\n{word_lowered}\")\n",
    "print(f\"\\nstop word removal:\\n{stop_wording}\")\n",
    "print(f\"\\nTokenised lowercase text:\\n{tokens}\")\n",
    "print(f\"\\nStemmed lowercase text:\\n{stemmed}\")\n",
    "print(f\"\\nStemmed lowercase sentence:\\n{' '.join(stemmed)}\")\n",
    "print(f\"\\nPOS tagging lowercase text:\\n{pos_tag(stemmed)}\")\n",
    "print(f\"\\nFrequency of occurrence: \\n{fdist3.most_common()}\") \n",
    "# The serves the same function as the FreqDist\n",
    "# print(f\"\\nFrequency of occurrence with counter: \\n{Counter(fdist3)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation approach version-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercase text\n",
      "during the conference, within the machine learning session, xiu tang, sai wu, gang chen, ke chen, and lidan shou of college of computer science and technology, zhejiang university, zhejiang, china, gave a presentation on the “learning to label with active learning and reinforcement learning”.  they pointed out that it is financially training data labelling in domain-specific learning applications, which relies on the intelligence from domain experts. they proposed a learning-to-label (l2l) framework leveraging active learning and reinforcement learning to iteratively select data to label for name entity recognition (ner) task. they pointed out that, neural models are built on top of few open datasets with well-defined labels, such as; imagenet, coco, and wikipedia dataset, which cannot be directly applied to new domain. this means that neural model using domain-specific labels must be trained, but this is quite expensive. to address the problem of lacking labeled training data, they proposed the use of transfer learning technique which establishes a model on the source domain and transfers the knowledge to a target domain, but with an extension by exploiting active learning and reinforcement learning techniques. by use of a label approach, l2l, which consists of two models, a transfer learning model and an active learning model designed by a reinforcement learning process named t-model and a-model, respectively, to rank the data for labelling for the name entity recognition (ner) application. the l2l architecture consisted of three main components which are: ner model, multi-granularity attention, and learning to rank. the proposed architecture reduced the required number of labels for training a domain-specific neural model. the idea of this model is to first transfer a learning model from a source domain to a target domain, and then apply the active learning to gradually improve the performance of the model using as few labeled data in the target domain as possible. their experimental results showed that their approach is more effective than strong previous methods using heuristics and reinforcement learning. with the same number of labeled data, their approach improved the accuracy of ner by 11.91%. moreover, this approach is superior to state-of-the-art learning- to-label method, with an improvement of accuracy by 6.49%.\n",
      "\n",
      "stop word removal:\n",
      "conference, within machine learning session, xiu tang, sai wu, gang chen, ke chen, lidan shou college computer science technology, zhejiang university, zhejiang, china, gave presentation “learning label active learning reinforcement learning”. pointed financially training data labelling domain-specific learning applications, relies intelligence domain experts. proposed learning-to-label (l2l) framework leveraging active learning reinforcement learning iteratively select data label name entity recognition (ner) task. pointed that, neural models built top open datasets well-defined labels, as; imagenet, coco, wikipedia dataset, cannot directly applied new domain. means neural model using domain-specific labels must trained, quite expensive. address problem lacking labeled training data, proposed use transfer learning technique establishes model source domain transfers knowledge target domain, extension exploiting active learning reinforcement learning techniques. use label approach, l2l, consists two models, transfer learning model active learning model designed reinforcement learning process named t-model a-model, respectively, rank data labelling name entity recognition (ner) application. l2l architecture consisted three main components are: ner model, multi-granularity attention, learning rank. proposed architecture reduced required number labels training domain-specific neural model. idea model first transfer learning model source domain target domain, apply active learning gradually improve performance model using labeled data target domain possible. experimental results showed approach effective strong previous methods using heuristics reinforcement learning. number labeled data, approach improved accuracy ner 11.91%. moreover, approach superior state-of-the-art learning- to-label method, improvement accuracy 6.49%.\n",
      "\n",
      "Tokenised lowercase text:\n",
      "['conference', ',', 'within', 'machine', 'learning', 'session', ',', 'xiu', 'tang', ',', 'sai', 'wu', ',', 'gang', 'chen', ',', 'ke', 'chen', ',', 'lidan', 'shou', 'college', 'computer', 'science', 'technology', ',', 'zhejiang', 'university', ',', 'zhejiang', ',', 'china', ',', 'gave', 'presentation', '“', 'learning', 'label', 'active', 'learning', 'reinforcement', 'learning', '”', '.', 'pointed', 'financially', 'training', 'data', 'labelling', 'domain-specific', 'learning', 'applications', ',', 'relies', 'intelligence', 'domain', 'experts', '.', 'proposed', 'learning-to-label', '(', 'l2l', ')', 'framework', 'leveraging', 'active', 'learning', 'reinforcement', 'learning', 'iteratively', 'select', 'data', 'label', 'name', 'entity', 'recognition', '(', 'ner', ')', 'task', '.', 'pointed', 'that', ',', 'neural', 'models', 'built', 'top', 'open', 'datasets', 'well-defined', 'labels', ',', 'as', ';', 'imagenet', ',', 'coco', ',', 'wikipedia', 'dataset', ',', 'can', 'not', 'directly', 'applied', 'new', 'domain', '.', 'means', 'neural', 'model', 'using', 'domain-specific', 'labels', 'must', 'trained', ',', 'quite', 'expensive', '.', 'address', 'problem', 'lacking', 'labeled', 'training', 'data', ',', 'proposed', 'use', 'transfer', 'learning', 'technique', 'establishes', 'model', 'source', 'domain', 'transfers', 'knowledge', 'target', 'domain', ',', 'extension', 'exploiting', 'active', 'learning', 'reinforcement', 'learning', 'techniques', '.', 'use', 'label', 'approach', ',', 'l2l', ',', 'consists', 'two', 'models', ',', 'transfer', 'learning', 'model', 'active', 'learning', 'model', 'designed', 'reinforcement', 'learning', 'process', 'named', 't-model', 'a-model', ',', 'respectively', ',', 'rank', 'data', 'labelling', 'name', 'entity', 'recognition', '(', 'ner', ')', 'application', '.', 'l2l', 'architecture', 'consisted', 'three', 'main', 'components', 'are', ':', 'ner', 'model', ',', 'multi-granularity', 'attention', ',', 'learning', 'rank', '.', 'proposed', 'architecture', 'reduced', 'required', 'number', 'labels', 'training', 'domain-specific', 'neural', 'model', '.', 'idea', 'model', 'first', 'transfer', 'learning', 'model', 'source', 'domain', 'target', 'domain', ',', 'apply', 'active', 'learning', 'gradually', 'improve', 'performance', 'model', 'using', 'labeled', 'data', 'target', 'domain', 'possible', '.', 'experimental', 'results', 'showed', 'approach', 'effective', 'strong', 'previous', 'methods', 'using', 'heuristics', 'reinforcement', 'learning', '.', 'number', 'labeled', 'data', ',', 'approach', 'improved', 'accuracy', 'ner', '11.91', '%', '.', 'moreover', ',', 'approach', 'superior', 'state-of-the-art', 'learning-', 'to-label', 'method', ',', 'improvement', 'accuracy', '6.49', '%', '.']\n",
      "\n",
      "Lemmatised lowercase text:\n",
      "['conference', ',', 'within', 'machine', 'learn', 'session', ',', 'xiu', 'tang', ',', 'sai', 'wu', ',', 'gang', 'chen', ',', 'ke', 'chen', ',', 'lidan', 'shou', 'college', 'computer', 'science', 'technology', ',', 'zhejiang', 'university', ',', 'zhejiang', ',', 'china', ',', 'give', 'presentation', '“', 'learn', 'label', 'active', 'learn', 'reinforcement', 'learn', '”', '.', 'point', 'financially', 'train', 'data', 'label', 'domain-specific', 'learn', 'applications', ',', 'rely', 'intelligence', 'domain', 'experts', '.', 'propose', 'learning-to-label', '(', 'l2l', ')', 'framework', 'leverage', 'active', 'learn', 'reinforcement', 'learn', 'iteratively', 'select', 'data', 'label', 'name', 'entity', 'recognition', '(', 'ner', ')', 'task', '.', 'point', 'that', ',', 'neural', 'model', 'build', 'top', 'open', 'datasets', 'well-defined', 'label', ',', 'as', ';', 'imagenet', ',', 'coco', ',', 'wikipedia', 'dataset', ',', 'can', 'not', 'directly', 'apply', 'new', 'domain', '.', 'mean', 'neural', 'model', 'use', 'domain-specific', 'label', 'must', 'train', ',', 'quite', 'expensive', '.', 'address', 'problem', 'lack', 'label', 'train', 'data', ',', 'propose', 'use', 'transfer', 'learn', 'technique', 'establish', 'model', 'source', 'domain', 'transfer', 'knowledge', 'target', 'domain', ',', 'extension', 'exploit', 'active', 'learn', 'reinforcement', 'learn', 'techniques', '.', 'use', 'label', 'approach', ',', 'l2l', ',', 'consist', 'two', 'model', ',', 'transfer', 'learn', 'model', 'active', 'learn', 'model', 'design', 'reinforcement', 'learn', 'process', 'name', 't-model', 'a-model', ',', 'respectively', ',', 'rank', 'data', 'label', 'name', 'entity', 'recognition', '(', 'ner', ')', 'application', '.', 'l2l', 'architecture', 'consist', 'three', 'main', 'components', 'be', ':', 'ner', 'model', ',', 'multi-granularity', 'attention', ',', 'learn', 'rank', '.', 'propose', 'architecture', 'reduce', 'require', 'number', 'label', 'train', 'domain-specific', 'neural', 'model', '.', 'idea', 'model', 'first', 'transfer', 'learn', 'model', 'source', 'domain', 'target', 'domain', ',', 'apply', 'active', 'learn', 'gradually', 'improve', 'performance', 'model', 'use', 'label', 'data', 'target', 'domain', 'possible', '.', 'experimental', 'result', 'show', 'approach', 'effective', 'strong', 'previous', 'methods', 'use', 'heuristics', 'reinforcement', 'learn', '.', 'number', 'label', 'data', ',', 'approach', 'improve', 'accuracy', 'ner', '11.91', '%', '.', 'moreover', ',', 'approach', 'superior', 'state-of-the-art', 'learning-', 'to-label', 'method', ',', 'improvement', 'accuracy', '6.49', '%', '.']\n",
      "\n",
      "Lemmatised lowercase sentence:\n",
      "conference , within machine learn session , xiu tang , sai wu , gang chen , ke chen , lidan shou college computer science technology , zhejiang university , zhejiang , china , give presentation “ learn label active learn reinforcement learn ” . point financially train data label domain-specific learn applications , rely intelligence domain experts . propose learning-to-label ( l2l ) framework leverage active learn reinforcement learn iteratively select data label name entity recognition ( ner ) task . point that , neural model build top open datasets well-defined label , as ; imagenet , coco , wikipedia dataset , can not directly apply new domain . mean neural model use domain-specific label must train , quite expensive . address problem lack label train data , propose use transfer learn technique establish model source domain transfer knowledge target domain , extension exploit active learn reinforcement learn techniques . use label approach , l2l , consist two model , transfer learn model active learn model design reinforcement learn process name t-model a-model , respectively , rank data label name entity recognition ( ner ) application . l2l architecture consist three main components be : ner model , multi-granularity attention , learn rank . propose architecture reduce require number label train domain-specific neural model . idea model first transfer learn model source domain target domain , apply active learn gradually improve performance model use label data target domain possible . experimental result show approach effective strong previous methods use heuristics reinforcement learn . number label data , approach improve accuracy ner 11.91 % . moreover , approach superior state-of-the-art learning- to-label method , improvement accuracy 6.49 % .\n",
      "\n",
      "POS tagging lowercase text:\n",
      "[('conference', 'NN'), (',', ','), ('within', 'IN'), ('machine', 'NN'), ('learn', 'NN'), ('session', 'NN'), (',', ','), ('xiu', 'NNP'), ('tang', 'NN'), (',', ','), ('sai', 'NN'), ('wu', 'NN'), (',', ','), ('gang', 'NN'), ('chen', 'NN'), (',', ','), ('ke', 'NN'), ('chen', 'NN'), (',', ','), ('lidan', 'JJ'), ('shou', 'NN'), ('college', 'NN'), ('computer', 'NN'), ('science', 'NN'), ('technology', 'NN'), (',', ','), ('zhejiang', 'NNP'), ('university', 'NN'), (',', ','), ('zhejiang', 'NN'), (',', ','), ('china', 'NN'), (',', ','), ('give', 'VB'), ('presentation', 'NN'), ('“', 'NNP'), ('learn', 'NN'), ('label', 'NN'), ('active', 'JJ'), ('learn', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('”', 'NNP'), ('.', '.'), ('point', 'NN'), ('financially', 'RB'), ('train', 'VB'), ('data', 'NNS'), ('label', 'RB'), ('domain-specific', 'NN'), ('learn', 'NN'), ('applications', 'NNS'), (',', ','), ('rely', 'RB'), ('intelligence', 'NN'), ('domain', 'NN'), ('experts', 'NNS'), ('.', '.'), ('propose', 'VB'), ('learning-to-label', 'JJ'), ('(', '('), ('l2l', 'NN'), (')', ')'), ('framework', 'NN'), ('leverage', 'NN'), ('active', 'JJ'), ('learn', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('iteratively', 'RB'), ('select', 'JJ'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('(', '('), ('ner', 'NN'), (')', ')'), ('task', 'NN'), ('.', '.'), ('point', 'NN'), ('that', 'IN'), (',', ','), ('neural', 'JJ'), ('model', 'NN'), ('build', 'NN'), ('top', 'JJ'), ('open', 'JJ'), ('datasets', 'NNS'), ('well-defined', 'JJ'), ('label', 'NN'), (',', ','), ('as', 'IN'), (';', ':'), ('imagenet', 'NN'), (',', ','), ('coco', 'NN'), (',', ','), ('wikipedia', 'NN'), ('dataset', 'NN'), (',', ','), ('can', 'MD'), ('not', 'RB'), ('directly', 'RB'), ('apply', 'VB'), ('new', 'JJ'), ('domain', 'NN'), ('.', '.'), ('mean', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('use', 'IN'), ('domain-specific', 'JJ'), ('label', 'NN'), ('must', 'MD'), ('train', 'VB'), (',', ','), ('quite', 'RB'), ('expensive', 'JJ'), ('.', '.'), ('address', 'NN'), ('problem', 'NN'), ('lack', 'NN'), ('label', 'NN'), ('train', 'NN'), ('data', 'NNS'), (',', ','), ('propose', 'NN'), ('use', 'NN'), ('transfer', 'NN'), ('learn', 'NN'), ('technique', 'NN'), ('establish', 'VB'), ('model', 'NN'), ('source', 'NN'), ('domain', 'NN'), ('transfer', 'NN'), ('knowledge', 'NN'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('extension', 'NN'), ('exploit', 'RB'), ('active', 'JJ'), ('learn', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('techniques', 'NNS'), ('.', '.'), ('use', 'NN'), ('label', 'JJ'), ('approach', 'NN'), (',', ','), ('l2l', 'NN'), (',', ','), ('consist', 'VBP'), ('two', 'CD'), ('model', 'NN'), (',', ','), ('transfer', 'NN'), ('learn', 'NN'), ('model', 'NN'), ('active', 'JJ'), ('learn', 'JJ'), ('model', 'NN'), ('design', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('process', 'NN'), ('name', 'IN'), ('t-model', 'JJ'), ('a-model', 'NN'), (',', ','), ('respectively', 'RB'), (',', ','), ('rank', 'NN'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('(', '('), ('ner', 'NN'), (')', ')'), ('application', 'NN'), ('.', '.'), ('l2l', 'JJ'), ('architecture', 'NN'), ('consist', 'VBP'), ('three', 'CD'), ('main', 'JJ'), ('components', 'NNS'), ('be', 'VB'), (':', ':'), ('ner', 'JJ'), ('model', 'NN'), (',', ','), ('multi-granularity', 'JJ'), ('attention', 'NN'), (',', ','), ('learn', 'JJ'), ('rank', 'NN'), ('.', '.'), ('propose', 'JJ'), ('architecture', 'NN'), ('reduce', 'VB'), ('require', 'NN'), ('number', 'NN'), ('label', 'JJ'), ('train', 'NN'), ('domain-specific', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('.', '.'), ('idea', 'NN'), ('model', 'NN'), ('first', 'JJ'), ('transfer', 'NN'), ('learn', 'NN'), ('model', 'NN'), ('source', 'NN'), ('domain', 'NN'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('apply', 'RB'), ('active', 'JJ'), ('learn', 'NN'), ('gradually', 'RB'), ('improve', 'VB'), ('performance', 'NN'), ('model', 'NN'), ('use', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('target', 'NN'), ('domain', 'NN'), ('possible', 'JJ'), ('.', '.'), ('experimental', 'JJ'), ('result', 'NN'), ('show', 'NN'), ('approach', 'NN'), ('effective', 'JJ'), ('strong', 'JJ'), ('previous', 'JJ'), ('methods', 'NNS'), ('use', 'VBP'), ('heuristics', 'NNS'), ('reinforcement', 'NN'), ('learn', 'NN'), ('.', '.'), ('number', 'NN'), ('label', 'NN'), ('data', 'NNS'), (',', ','), ('approach', 'NN'), ('improve', 'VB'), ('accuracy', 'NN'), ('ner', 'RB'), ('11.91', 'CD'), ('%', 'NN'), ('.', '.'), ('moreover', 'NN'), (',', ','), ('approach', 'NN'), ('superior', 'JJ'), ('state-of-the-art', 'JJ'), ('learning-', 'JJ'), ('to-label', 'NN'), ('method', 'NN'), (',', ','), ('improvement', 'NN'), ('accuracy', 'NN'), ('6.49', 'CD'), ('%', 'NN'), ('.', '.')]\n",
      "\n",
      "Frequency of occurrence: \n",
      "[(',', 30), ('learn', 17), ('.', 13), ('label', 11), ('model', 11), ('domain', 7), ('data', 6), ('active', 5), ('reinforcement', 5), ('use', 5), ('train', 4), ('ner', 4), ('transfer', 4), ('approach', 4), ('domain-specific', 3), ('propose', 3), ('(', 3), ('l2l', 3), (')', 3), ('name', 3), ('neural', 3), ('target', 3), ('chen', 2), ('zhejiang', 2), ('point', 2), ('entity', 2), ('recognition', 2), ('apply', 2), ('source', 2), ('consist', 2), ('rank', 2), ('architecture', 2), ('number', 2), ('improve', 2), ('accuracy', 2), ('%', 2), ('conference', 1), ('within', 1), ('machine', 1), ('session', 1), ('xiu', 1), ('tang', 1), ('sai', 1), ('wu', 1), ('gang', 1), ('ke', 1), ('lidan', 1), ('shou', 1), ('college', 1), ('computer', 1), ('science', 1), ('technology', 1), ('university', 1), ('china', 1), ('give', 1), ('presentation', 1), ('“', 1), ('”', 1), ('financially', 1), ('applications', 1), ('rely', 1), ('intelligence', 1), ('experts', 1), ('learning-to-label', 1), ('framework', 1), ('leverage', 1), ('iteratively', 1), ('select', 1), ('task', 1), ('that', 1), ('build', 1), ('top', 1), ('open', 1), ('datasets', 1), ('well-defined', 1), ('as', 1), (';', 1), ('imagenet', 1), ('coco', 1), ('wikipedia', 1), ('dataset', 1), ('can', 1), ('not', 1), ('directly', 1), ('new', 1), ('mean', 1), ('must', 1), ('quite', 1), ('expensive', 1), ('address', 1), ('problem', 1), ('lack', 1), ('technique', 1), ('establish', 1), ('knowledge', 1), ('extension', 1), ('exploit', 1), ('techniques', 1), ('two', 1), ('design', 1), ('process', 1), ('t-model', 1), ('a-model', 1), ('respectively', 1), ('application', 1), ('three', 1), ('main', 1), ('components', 1), ('be', 1), (':', 1), ('multi-granularity', 1), ('attention', 1), ('reduce', 1), ('require', 1), ('idea', 1), ('first', 1), ('gradually', 1), ('performance', 1), ('possible', 1), ('experimental', 1), ('result', 1), ('show', 1), ('effective', 1), ('strong', 1), ('previous', 1), ('methods', 1), ('heuristics', 1), ('11.91', 1), ('moreover', 1), ('superior', 1), ('state-of-the-art', 1), ('learning-', 1), ('to-label', 1), ('method', 1), ('improvement', 1), ('6.49', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation-1 approach\n",
    "\n",
    "# lowercased text\n",
    "word_lowered = wordlowercase(raw_text)\n",
    "\n",
    "# removal of stopwords\n",
    "stop_wording = stopwordremove2(word_lowered)\n",
    "\n",
    "# tokenisation\n",
    "tokens = word_tokenise2(stop_wording)\n",
    "\n",
    "# Lemmatisation\n",
    "lemma = lemmtiser_verb(tokens) #verbs\n",
    "# lemma = lemmtiser_adjective(tokens) # adjectives\n",
    "# lemma = lemmtiser_noun(tokens) # nouns\n",
    "\n",
    "\n",
    "# Frequency of occurrence\n",
    "fdist3 = FreqDist(lemma)\n",
    "\n",
    "\n",
    "print(f\"\\nLowercase text\\n{word_lowered}\")\n",
    "print(f\"\\nstop word removal:\\n{stop_wording}\")\n",
    "print(f\"\\nTokenised lowercase text:\\n{tokens}\")\n",
    "print(f\"\\nLemmatised lowercase text:\\n{lemma}\")\n",
    "print(f\"\\nLemmatised lowercase sentence:\\n{' '.join(lemma)}\")\n",
    "print(f\"\\nPOS tagging lowercase text:\\n{pos_tag(lemma)}\")\n",
    "print(f\"\\nFrequency of occurrence: \\n{fdist3.most_common()}\") \n",
    "# print(f\"\\nFrequency of occurrence with counter: \\n{Counter(fdist3)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation approach version-2: using POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_pos = {\n",
    "    # Nouns\n",
    "    'NN':wordnet.NOUN,\n",
    "    # Verbs\n",
    "    'VB':wordnet.VERB,\n",
    "    # Adjectives\n",
    "    'JJ':wordnet.ADJ,\n",
    "    # Adverbs\n",
    "    'RB':wordnet.ADV,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lowercase text\n",
      "during the conference, within the machine learning session, xiu tang, sai wu, gang chen, ke chen, and lidan shou of college of computer science and technology, zhejiang university, zhejiang, china, gave a presentation on the “learning to label with active learning and reinforcement learning”.  they pointed out that it is financially training data labelling in domain-specific learning applications, which relies on the intelligence from domain experts. they proposed a learning-to-label (l2l) framework leveraging active learning and reinforcement learning to iteratively select data to label for name entity recognition (ner) task. they pointed out that, neural models are built on top of few open datasets with well-defined labels, such as; imagenet, coco, and wikipedia dataset, which cannot be directly applied to new domain. this means that neural model using domain-specific labels must be trained, but this is quite expensive. to address the problem of lacking labeled training data, they proposed the use of transfer learning technique which establishes a model on the source domain and transfers the knowledge to a target domain, but with an extension by exploiting active learning and reinforcement learning techniques. by use of a label approach, l2l, which consists of two models, a transfer learning model and an active learning model designed by a reinforcement learning process named t-model and a-model, respectively, to rank the data for labelling for the name entity recognition (ner) application. the l2l architecture consisted of three main components which are: ner model, multi-granularity attention, and learning to rank. the proposed architecture reduced the required number of labels for training a domain-specific neural model. the idea of this model is to first transfer a learning model from a source domain to a target domain, and then apply the active learning to gradually improve the performance of the model using as few labeled data in the target domain as possible. their experimental results showed that their approach is more effective than strong previous methods using heuristics and reinforcement learning. with the same number of labeled data, their approach improved the accuracy of ner by 11.91%. moreover, this approach is superior to state-of-the-art learning- to-label method, with an improvement of accuracy by 6.49%.\n",
      "\n",
      "stop word removal:\n",
      "conference, within machine learning session, xiu tang, sai wu, gang chen, ke chen, lidan shou college computer science technology, zhejiang university, zhejiang, china, gave presentation “learning label active learning reinforcement learning”. pointed financially training data labelling domain-specific learning applications, relies intelligence domain experts. proposed learning-to-label (l2l) framework leveraging active learning reinforcement learning iteratively select data label name entity recognition (ner) task. pointed that, neural models built top open datasets well-defined labels, as; imagenet, coco, wikipedia dataset, cannot directly applied new domain. means neural model using domain-specific labels must trained, quite expensive. address problem lacking labeled training data, proposed use transfer learning technique establishes model source domain transfers knowledge target domain, extension exploiting active learning reinforcement learning techniques. use label approach, l2l, consists two models, transfer learning model active learning model designed reinforcement learning process named t-model a-model, respectively, rank data labelling name entity recognition (ner) application. l2l architecture consisted three main components are: ner model, multi-granularity attention, learning rank. proposed architecture reduced required number labels training domain-specific neural model. idea model first transfer learning model source domain target domain, apply active learning gradually improve performance model using labeled data target domain possible. experimental results showed approach effective strong previous methods using heuristics reinforcement learning. number labeled data, approach improved accuracy ner 11.91%. moreover, approach superior state-of-the-art learning- to-label method, improvement accuracy 6.49%.\n",
      "\n",
      "Tokenised lowercase text:\n",
      "['conference', ',', 'within', 'machine', 'learning', 'session', ',', 'xiu', 'tang', ',', 'sai', 'wu', ',', 'gang', 'chen', ',', 'ke', 'chen', ',', 'lidan', 'shou', 'college', 'computer', 'science', 'technology', ',', 'zhejiang', 'university', ',', 'zhejiang', ',', 'china', ',', 'gave', 'presentation', '“', 'learning', 'label', 'active', 'learning', 'reinforcement', 'learning', '”', '.', 'pointed', 'financially', 'training', 'data', 'labelling', 'domain-specific', 'learning', 'applications', ',', 'relies', 'intelligence', 'domain', 'experts', '.', 'proposed', 'learning-to-label', '(', 'l2l', ')', 'framework', 'leveraging', 'active', 'learning', 'reinforcement', 'learning', 'iteratively', 'select', 'data', 'label', 'name', 'entity', 'recognition', '(', 'ner', ')', 'task', '.', 'pointed', 'that', ',', 'neural', 'models', 'built', 'top', 'open', 'datasets', 'well-defined', 'labels', ',', 'as', ';', 'imagenet', ',', 'coco', ',', 'wikipedia', 'dataset', ',', 'can', 'not', 'directly', 'applied', 'new', 'domain', '.', 'means', 'neural', 'model', 'using', 'domain-specific', 'labels', 'must', 'trained', ',', 'quite', 'expensive', '.', 'address', 'problem', 'lacking', 'labeled', 'training', 'data', ',', 'proposed', 'use', 'transfer', 'learning', 'technique', 'establishes', 'model', 'source', 'domain', 'transfers', 'knowledge', 'target', 'domain', ',', 'extension', 'exploiting', 'active', 'learning', 'reinforcement', 'learning', 'techniques', '.', 'use', 'label', 'approach', ',', 'l2l', ',', 'consists', 'two', 'models', ',', 'transfer', 'learning', 'model', 'active', 'learning', 'model', 'designed', 'reinforcement', 'learning', 'process', 'named', 't-model', 'a-model', ',', 'respectively', ',', 'rank', 'data', 'labelling', 'name', 'entity', 'recognition', '(', 'ner', ')', 'application', '.', 'l2l', 'architecture', 'consisted', 'three', 'main', 'components', 'are', ':', 'ner', 'model', ',', 'multi-granularity', 'attention', ',', 'learning', 'rank', '.', 'proposed', 'architecture', 'reduced', 'required', 'number', 'labels', 'training', 'domain-specific', 'neural', 'model', '.', 'idea', 'model', 'first', 'transfer', 'learning', 'model', 'source', 'domain', 'target', 'domain', ',', 'apply', 'active', 'learning', 'gradually', 'improve', 'performance', 'model', 'using', 'labeled', 'data', 'target', 'domain', 'possible', '.', 'experimental', 'results', 'showed', 'approach', 'effective', 'strong', 'previous', 'methods', 'using', 'heuristics', 'reinforcement', 'learning', '.', 'number', 'labeled', 'data', ',', 'approach', 'improved', 'accuracy', 'ner', '11.91', '%', '.', 'moreover', ',', 'approach', 'superior', 'state-of-the-art', 'learning-', 'to-label', 'method', ',', 'improvement', 'accuracy', '6.49', '%', '.']\n",
      "\n",
      "Tagged tokens:\n",
      "[('conference', 'NN'), (',', ','), ('within', 'IN'), ('machine', 'NN'), ('learning', 'NN'), ('session', 'NN'), (',', ','), ('xiu', 'NNP'), ('tang', 'NN'), (',', ','), ('sai', 'NN'), ('wu', 'NN'), (',', ','), ('gang', 'NN'), ('chen', 'NN'), (',', ','), ('ke', 'NN'), ('chen', 'NN'), (',', ','), ('lidan', 'JJ'), ('shou', 'NN'), ('college', 'NN'), ('computer', 'NN'), ('science', 'NN'), ('technology', 'NN'), (',', ','), ('zhejiang', 'NNP'), ('university', 'NN'), (',', ','), ('zhejiang', 'NN'), (',', ','), ('china', 'NN'), (',', ','), ('gave', 'VBD'), ('presentation', 'NN'), ('“', 'NN'), ('learning', 'VBG'), ('label', 'JJ'), ('active', 'JJ'), ('learning', 'NN'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('”', 'NNP'), ('.', '.'), ('pointed', 'VBN'), ('financially', 'RB'), ('training', 'VBG'), ('data', 'NNS'), ('labelling', 'VBG'), ('domain-specific', 'JJ'), ('learning', 'NN'), ('applications', 'NNS'), (',', ','), ('relies', 'NNS'), ('intelligence', 'VBP'), ('domain', 'NN'), ('experts', 'NNS'), ('.', '.'), ('proposed', 'VBN'), ('learning-to-label', 'NN'), ('(', '('), ('l2l', 'NN'), (')', ')'), ('framework', 'NN'), ('leveraging', 'VBG'), ('active', 'JJ'), ('learning', 'JJ'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('iteratively', 'RB'), ('select', 'JJ'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('(', '('), ('ner', 'NN'), (')', ')'), ('task', 'NN'), ('.', '.'), ('pointed', 'VBD'), ('that', 'IN'), (',', ','), ('neural', 'JJ'), ('models', 'NNS'), ('built', 'VBN'), ('top', 'JJ'), ('open', 'JJ'), ('datasets', 'NNS'), ('well-defined', 'JJ'), ('labels', 'NNS'), (',', ','), ('as', 'IN'), (';', ':'), ('imagenet', 'NN'), (',', ','), ('coco', 'NN'), (',', ','), ('wikipedia', 'NN'), ('dataset', 'NN'), (',', ','), ('can', 'MD'), ('not', 'RB'), ('directly', 'RB'), ('applied', 'VBN'), ('new', 'JJ'), ('domain', 'NN'), ('.', '.'), ('means', 'VBZ'), ('neural', 'JJ'), ('model', 'NN'), ('using', 'VBG'), ('domain-specific', 'JJ'), ('labels', 'NNS'), ('must', 'MD'), ('trained', 'VB'), (',', ','), ('quite', 'RB'), ('expensive', 'JJ'), ('.', '.'), ('address', 'NN'), ('problem', 'NN'), ('lacking', 'VBG'), ('labeled', 'VBD'), ('training', 'NN'), ('data', 'NNS'), (',', ','), ('proposed', 'VBN'), ('use', 'NN'), ('transfer', 'NN'), ('learning', 'NN'), ('technique', 'NN'), ('establishes', 'VBZ'), ('model', 'FW'), ('source', 'NN'), ('domain', 'NN'), ('transfers', 'NNS'), ('knowledge', 'VBP'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('extension', 'NN'), ('exploiting', 'VBG'), ('active', 'JJ'), ('learning', 'JJ'), ('reinforcement', 'NN'), ('learning', 'VBG'), ('techniques', 'NNS'), ('.', '.'), ('use', 'NN'), ('label', 'JJ'), ('approach', 'NN'), (',', ','), ('l2l', 'NN'), (',', ','), ('consists', 'VBZ'), ('two', 'CD'), ('models', 'NNS'), (',', ','), ('transfer', 'VB'), ('learning', 'VBG'), ('model', 'RB'), ('active', 'JJ'), ('learning', 'VBG'), ('model', 'NN'), ('designed', 'VBN'), ('reinforcement', 'JJ'), ('learning', 'VBG'), ('process', 'NN'), ('named', 'VBN'), ('t-model', 'JJ'), ('a-model', 'NN'), (',', ','), ('respectively', 'RB'), (',', ','), ('rank', 'NN'), ('data', 'NNS'), ('labelling', 'VBG'), ('name', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('(', '('), ('ner', 'NN'), (')', ')'), ('application', 'NN'), ('.', '.'), ('l2l', 'JJ'), ('architecture', 'NN'), ('consisted', 'VBD'), ('three', 'CD'), ('main', 'JJ'), ('components', 'NNS'), ('are', 'VBP'), (':', ':'), ('ner', 'JJ'), ('model', 'NN'), (',', ','), ('multi-granularity', 'JJ'), ('attention', 'NN'), (',', ','), ('learning', 'VBG'), ('rank', 'NN'), ('.', '.'), ('proposed', 'VBN'), ('architecture', 'NN'), ('reduced', 'VBN'), ('required', 'JJ'), ('number', 'NN'), ('labels', 'NNS'), ('training', 'VBG'), ('domain-specific', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('.', '.'), ('idea', 'NN'), ('model', 'NN'), ('first', 'JJ'), ('transfer', 'NN'), ('learning', 'VBG'), ('model', 'NN'), ('source', 'NN'), ('domain', 'NN'), ('target', 'NN'), ('domain', 'NN'), (',', ','), ('apply', 'RB'), ('active', 'JJ'), ('learning', 'VBG'), ('gradually', 'RB'), ('improve', 'VB'), ('performance', 'NN'), ('model', 'NN'), ('using', 'VBG'), ('labeled', 'VBN'), ('data', 'NNS'), ('target', 'NN'), ('domain', 'NN'), ('possible', 'JJ'), ('.', '.'), ('experimental', 'JJ'), ('results', 'NNS'), ('showed', 'VBD'), ('approach', 'JJ'), ('effective', 'JJ'), ('strong', 'JJ'), ('previous', 'JJ'), ('methods', 'NNS'), ('using', 'VBG'), ('heuristics', 'NNS'), ('reinforcement', 'JJ'), ('learning', 'VBG'), ('.', '.'), ('number', 'NN'), ('labeled', 'VBN'), ('data', 'NNS'), (',', ','), ('approach', 'NN'), ('improved', 'VBD'), ('accuracy', 'JJ'), ('ner', 'RB'), ('11.91', 'CD'), ('%', 'NN'), ('.', '.'), ('moreover', 'NN'), (',', ','), ('approach', 'NN'), ('superior', 'JJ'), ('state-of-the-art', 'JJ'), ('learning-', 'JJ'), ('to-label', 'NN'), ('method', 'NN'), (',', ','), ('improvement', 'NN'), ('accuracy', 'NN'), ('6.49', 'CD'), ('%', 'NN'), ('.', '.')]\n",
      "\n",
      "Lemmatised words:\n",
      "['conference', 'machine', 'learning', 'session', 'tang', 'sai', 'wu', 'gang', 'chen', 'ke', 'chen', 'lidan', 'shou', 'college', 'computer', 'science', 'technology', 'university', 'zhejiang', 'china', 'give', 'presentation', '“', 'learn', 'label', 'active', 'learning', 'reinforcement', 'learn', 'point', 'financially', 'train', 'data', 'label', 'domain-specific', 'learning', 'application', 'relies', 'intelligence', 'domain', 'expert', 'propose', 'learning-to-label', 'l2l', 'framework', 'leverage', 'active', 'learning', 'reinforcement', 'learn', 'iteratively', 'select', 'data', 'label', 'name', 'entity', 'recognition', 'ner', 'task', 'point', 'neural', 'model', 'build', 'top', 'open', 'datasets', 'well-defined', 'label', 'imagenet', 'coco', 'wikipedia', 'dataset', 'not', 'directly', 'apply', 'new', 'domain', 'mean', 'neural', 'model', 'use', 'domain-specific', 'label', 'train', 'quite', 'expensive', 'address', 'problem', 'lack', 'label', 'training', 'data', 'propose', 'use', 'transfer', 'learning', 'technique', 'establish', 'source', 'domain', 'transfer', 'knowledge', 'target', 'domain', 'extension', 'exploit', 'active', 'learning', 'reinforcement', 'learn', 'technique', 'use', 'label', 'approach', 'l2l', 'consist', 'model', 'transfer', 'learn', 'model', 'active', 'learn', 'model', 'design', 'reinforcement', 'learn', 'process', 'name', 't-model', 'a-model', 'respectively', 'rank', 'data', 'label', 'name', 'entity', 'recognition', 'ner', 'application', 'l2l', 'architecture', 'consist', 'main', 'component', 'be', 'ner', 'model', 'multi-granularity', 'attention', 'learn', 'rank', 'propose', 'architecture', 'reduce', 'required', 'number', 'label', 'train', 'domain-specific', 'neural', 'model', 'idea', 'model', 'first', 'transfer', 'learn', 'model', 'source', 'domain', 'target', 'domain', 'apply', 'active', 'learn', 'gradually', 'improve', 'performance', 'model', 'use', 'label', 'data', 'target', 'domain', 'possible', 'experimental', 'result', 'show', 'approach', 'effective', 'strong', 'previous', 'method', 'use', 'heuristic', 'reinforcement', 'learn', 'number', 'label', 'data', 'approach', 'improve', 'accuracy', 'ner', '%', 'moreover', 'approach', 'superior', 'state-of-the-art', 'learning-', 'to-label', 'method', 'improvement', 'accuracy', '%']\n",
      "\n",
      "Lemmatised lowercase sentence:\n",
      "conference machine learning session tang sai wu gang chen ke chen lidan shou college computer science technology university zhejiang china give presentation “ learn label active learning reinforcement learn point financially train data label domain-specific learning application relies intelligence domain expert propose learning-to-label l2l framework leverage active learning reinforcement learn iteratively select data label name entity recognition ner task point neural model build top open datasets well-defined label imagenet coco wikipedia dataset not directly apply new domain mean neural model use domain-specific label train quite expensive address problem lack label training data propose use transfer learning technique establish source domain transfer knowledge target domain extension exploit active learning reinforcement learn technique use label approach l2l consist model transfer learn model active learn model design reinforcement learn process name t-model a-model respectively rank data label name entity recognition ner application l2l architecture consist main component be ner model multi-granularity attention learn rank propose architecture reduce required number label train domain-specific neural model idea model first transfer learn model source domain target domain apply active learn gradually improve performance model use label data target domain possible experimental result show approach effective strong previous method use heuristic reinforcement learn number label data approach improve accuracy ner % moreover approach superior state-of-the-art learning- to-label method improvement accuracy %\n",
      "\n",
      "POS tagging of lemmatised words:\n",
      "[('conference', 'NN'), ('machine', 'NN'), ('learning', 'VBG'), ('session', 'NN'), ('tang', 'NN'), ('sai', 'NN'), ('wu', 'NN'), ('gang', 'NN'), ('chen', 'NN'), ('ke', 'NN'), ('chen', 'NN'), ('lidan', 'VBZ'), ('shou', 'JJ'), ('college', 'NN'), ('computer', 'NN'), ('science', 'NN'), ('technology', 'NN'), ('university', 'NN'), ('zhejiang', 'NNP'), ('china', 'NN'), ('give', 'JJ'), ('presentation', 'NN'), ('“', 'NNP'), ('learn', 'NN'), ('label', 'NN'), ('active', 'JJ'), ('learning', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('point', 'NN'), ('financially', 'RB'), ('train', 'VB'), ('data', 'NNS'), ('label', 'RB'), ('domain-specific', 'JJ'), ('learning', 'VBG'), ('application', 'NN'), ('relies', 'NNS'), ('intelligence', 'NN'), ('domain', 'NN'), ('expert', 'JJ'), ('propose', 'JJ'), ('learning-to-label', 'JJ'), ('l2l', 'NN'), ('framework', 'NN'), ('leverage', 'NN'), ('active', 'JJ'), ('learning', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('iteratively', 'RB'), ('select', 'JJ'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('ner', 'NN'), ('task', 'NN'), ('point', 'NN'), ('neural', 'JJ'), ('model', 'NN'), ('build', 'NN'), ('top', 'JJ'), ('open', 'JJ'), ('datasets', 'NNS'), ('well-defined', 'JJ'), ('label', 'JJ'), ('imagenet', 'NN'), ('coco', 'NN'), ('wikipedia', 'NN'), ('dataset', 'NN'), ('not', 'RB'), ('directly', 'RB'), ('apply', 'VB'), ('new', 'JJ'), ('domain', 'NN'), ('mean', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('use', 'IN'), ('domain-specific', 'JJ'), ('label', 'NN'), ('train', 'NN'), ('quite', 'RB'), ('expensive', 'JJ'), ('address', 'NN'), ('problem', 'NN'), ('lack', 'NN'), ('label', 'NN'), ('training', 'NN'), ('data', 'NNS'), ('propose', 'NN'), ('use', 'NN'), ('transfer', 'NN'), ('learning', 'NN'), ('technique', 'NN'), ('establish', 'VB'), ('source', 'NN'), ('domain', 'NN'), ('transfer', 'NN'), ('knowledge', 'NN'), ('target', 'NN'), ('domain', 'NN'), ('extension', 'NN'), ('exploit', 'FW'), ('active', 'JJ'), ('learning', 'VBG'), ('reinforcement', 'NN'), ('learn', 'NN'), ('technique', 'NN'), ('use', 'NN'), ('label', 'JJ'), ('approach', 'NN'), ('l2l', 'JJ'), ('consist', 'JJ'), ('model', 'NN'), ('transfer', 'NN'), ('learn', 'VBP'), ('model', 'NN'), ('active', 'JJ'), ('learn', 'JJ'), ('model', 'NN'), ('design', 'NN'), ('reinforcement', 'NN'), ('learn', 'NN'), ('process', 'NN'), ('name', 'IN'), ('t-model', 'JJ'), ('a-model', 'NN'), ('respectively', 'RB'), ('rank', 'JJ'), ('data', 'NNS'), ('label', 'NN'), ('name', 'NN'), ('entity', 'NN'), ('recognition', 'NN'), ('ner', 'POS'), ('application', 'NN'), ('l2l', 'NN'), ('architecture', 'NN'), ('consist', 'NN'), ('main', 'JJ'), ('component', 'NN'), ('be', 'VB'), ('ner', 'JJ'), ('model', 'RBS'), ('multi-granularity', 'JJ'), ('attention', 'NN'), ('learn', 'NN'), ('rank', 'NN'), ('propose', 'JJ'), ('architecture', 'NN'), ('reduce', 'VB'), ('required', 'VBN'), ('number', 'NN'), ('label', 'NN'), ('train', 'VBP'), ('domain-specific', 'JJ'), ('neural', 'JJ'), ('model', 'NN'), ('idea', 'NN'), ('model', 'NN'), ('first', 'JJ'), ('transfer', 'NN'), ('learn', 'NN'), ('model', 'NN'), ('source', 'NN'), ('domain', 'NN'), ('target', 'NN'), ('domain', 'NN'), ('apply', 'RB'), ('active', 'JJ'), ('learn', 'NN'), ('gradually', 'RB'), ('improve', 'VB'), ('performance', 'NN'), ('model', 'NN'), ('use', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('target', 'NN'), ('domain', 'NN'), ('possible', 'JJ'), ('experimental', 'JJ'), ('result', 'NN'), ('show', 'NN'), ('approach', 'NN'), ('effective', 'JJ'), ('strong', 'JJ'), ('previous', 'JJ'), ('method', 'NN'), ('use', 'NN'), ('heuristic', 'JJ'), ('reinforcement', 'NN'), ('learn', 'NN'), ('number', 'NN'), ('label', 'NN'), ('data', 'NNS'), ('approach', 'NN'), ('improve', 'VB'), ('accuracy', 'NN'), ('ner', 'RB'), ('%', 'NN'), ('moreover', 'JJ'), ('approach', 'NN'), ('superior', 'JJ'), ('state-of-the-art', 'JJ'), ('learning-', 'JJ'), ('to-label', 'JJ'), ('method', 'NN'), ('improvement', 'NN'), ('accuracy', 'NN'), ('%', 'NN')]\n",
      "\n",
      "Frequency of occurrence: \n",
      "[('learn', 11), ('label', 11), ('model', 10), ('domain', 7), ('learning', 6), ('data', 6), ('active', 5), ('reinforcement', 5), ('use', 5), ('ner', 4), ('transfer', 4), ('approach', 4), ('train', 3), ('domain-specific', 3), ('propose', 3), ('l2l', 3), ('name', 3), ('neural', 3), ('target', 3), ('chen', 2), ('point', 2), ('application', 2), ('entity', 2), ('recognition', 2), ('apply', 2), ('technique', 2), ('source', 2), ('consist', 2), ('rank', 2), ('architecture', 2), ('number', 2), ('improve', 2), ('method', 2), ('accuracy', 2), ('%', 2), ('conference', 1), ('machine', 1), ('session', 1), ('tang', 1), ('sai', 1), ('wu', 1), ('gang', 1), ('ke', 1), ('lidan', 1), ('shou', 1), ('college', 1), ('computer', 1), ('science', 1), ('technology', 1), ('university', 1), ('zhejiang', 1), ('china', 1), ('give', 1), ('presentation', 1), ('“', 1), ('financially', 1), ('relies', 1), ('intelligence', 1), ('expert', 1), ('learning-to-label', 1), ('framework', 1), ('leverage', 1), ('iteratively', 1), ('select', 1), ('task', 1), ('build', 1), ('top', 1), ('open', 1), ('datasets', 1), ('well-defined', 1), ('imagenet', 1), ('coco', 1), ('wikipedia', 1), ('dataset', 1), ('not', 1), ('directly', 1), ('new', 1), ('mean', 1), ('quite', 1), ('expensive', 1), ('address', 1), ('problem', 1), ('lack', 1), ('training', 1), ('establish', 1), ('knowledge', 1), ('extension', 1), ('exploit', 1), ('design', 1), ('process', 1), ('t-model', 1), ('a-model', 1), ('respectively', 1), ('main', 1), ('component', 1), ('be', 1), ('multi-granularity', 1), ('attention', 1), ('reduce', 1), ('required', 1), ('idea', 1), ('first', 1), ('gradually', 1), ('performance', 1), ('possible', 1), ('experimental', 1), ('result', 1), ('show', 1), ('effective', 1), ('strong', 1), ('previous', 1), ('heuristic', 1), ('moreover', 1), ('superior', 1), ('state-of-the-art', 1), ('learning-', 1), ('to-label', 1), ('improvement', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatisation-2 approach\n",
    "\n",
    "lemmaword = WordNetLemmatizer()\n",
    "# lowercased text\n",
    "word_lowered = wordlowercase(raw_text)\n",
    "\n",
    "# removal of stopwords\n",
    "stop_wording = stopwordremove2(word_lowered)\n",
    "\n",
    "# tokenisation\n",
    "tokens = word_tokenise2(stop_wording)\n",
    "\n",
    "# tagging the tokens\n",
    "pos_tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "normalised_sequence = []\n",
    "for tuples in pos_tagged_tokens:\n",
    "    temp = tuples[0]\n",
    "    if tuples[1] == \"NNP\" or tuples[1] == \"NNPS\":\n",
    "        continue\n",
    "    elif tuples[1][:2] in mapped_pos.keys():\n",
    "        temp = lemmaword.lemmatize(tuples[0],pos=mapped_pos[tuples[1][:2]])\n",
    "        normalised_sequence.append(temp)\n",
    "\n",
    "# Frequency of occurrence\n",
    "fdist3 = FreqDist(normalised_sequence)\n",
    "\n",
    "\n",
    "print(f\"\\nLowercase text\\n{word_lowered}\")\n",
    "print(f\"\\nstop word removal:\\n{stop_wording}\")\n",
    "print(f\"\\nTokenised lowercase text:\\n{tokens}\")\n",
    "print(f\"\\nTagged tokens:\\n{pos_tagged_tokens}\")\n",
    "print(f\"\\nLemmatised words:\\n{normalised_sequence}\")\n",
    "print(f\"\\nLemmatised lowercase sentence:\\n{' '.join(normalised_sequence)}\")\n",
    "print(f\"\\nPOS tagging of lemmatised words:\\n{pos_tag(normalised_sequence)}\")\n",
    "print(f\"\\nFrequency of occurrence: \\n{fdist3.most_common()}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d3784fccdc90acbf957f8297e7e306d4c8b14c1a207bd5307d0795df9a8d77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
